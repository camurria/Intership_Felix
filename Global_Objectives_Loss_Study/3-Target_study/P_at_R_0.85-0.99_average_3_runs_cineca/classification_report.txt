                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5576888512372971   
                Loss on test data: 0.5629889732599258    
                                         
                0.85
              precision    recall  f1-score   support

           0     0.7977    0.9816    0.8801      5000
           1     0.9887    0.8238    0.8988      5000
           2     0.9735    0.9114    0.9414      5000

    accuracy                         0.9056     15000
   macro avg     0.9200    0.9056    0.9068     15000
weighted avg     0.9200    0.9056    0.9068     15000

matrix: 
 [[4908   14   78]
 [ 835 4119   46]
 [ 410   33 4557]]
matrix1: 
 [[98.16  0.28  1.56]
 [16.7  82.38  0.92]
 [ 8.2   0.66 91.14]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5568454322814942   
                Loss on test data: 0.561449961066246    
                                         
                0.85
              precision    recall  f1-score   support

           0     0.8182    0.9702    0.8877      5000
           1     0.9649    0.8904    0.9261      5000
           2     0.9825    0.8758    0.9261      5000

    accuracy                         0.9121     15000
   macro avg     0.9219    0.9121    0.9133     15000
weighted avg     0.9219    0.9121    0.9133     15000

matrix: 
 [[4851   95   54]
 [ 524 4452   24]
 [ 554   67 4379]]
matrix1: 
 [[97.02  1.9   1.08]
 [10.48 89.04  0.48]
 [11.08  1.34 87.58]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5678988679647445   
                Loss on test data: 0.5714403460025788    
                                         
                0.85
              precision    recall  f1-score   support

           0     0.7630    0.9728    0.8552      5000
           1     0.9535    0.8892    0.9202      5000
           2     0.9879    0.7828    0.8735      5000

    accuracy                         0.8816     15000
   macro avg     0.9014    0.8816    0.8830     15000
weighted avg     0.9014    0.8816    0.8830     15000

matrix: 
 [[4864  105   31]
 [ 537 4446   17]
 [ 974  112 3914]]
matrix1: 
 [[97.28  2.1   0.62]
 [10.74 88.92  0.34]
 [19.48  2.24 78.28]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5676002457141877   
                Loss on test data: 0.5709493860006333    
                                         
                0.86
              precision    recall  f1-score   support

           0     0.7742    0.9776    0.8641      5000
           1     0.9621    0.8830    0.9208      5000
           2     0.9861    0.8080    0.8882      5000

    accuracy                         0.8895     15000
   macro avg     0.9074    0.8895    0.8910     15000
weighted avg     0.9074    0.8895    0.8910     15000

matrix: 
 [[4888   77   35]
 [ 563 4415   22]
 [ 863   97 4040]]
matrix1: 
 [[97.76  1.54  0.7 ]
 [11.26 88.3   0.44]
 [17.26  1.94 80.8 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5582644114494324   
                Loss on test data: 0.5632919960021973    
                                         
                0.86
              precision    recall  f1-score   support

           0     0.8337    0.9632    0.8938      5000
           1     0.9505    0.8868    0.9175      5000
           2     0.9785    0.8920    0.9332      5000

    accuracy                         0.9140     15000
   macro avg     0.9209    0.9140    0.9148     15000
weighted avg     0.9209    0.9140    0.9148     15000

matrix: 
 [[4816  116   68]
 [ 536 4434   30]
 [ 425  115 4460]]
matrix1: 
 [[96.32  2.32  1.36]
 [10.72 88.68  0.6 ]
 [ 8.5   2.3  89.2 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5569047485589981   
                Loss on test data: 0.5624533601999283    
                                         
                0.86
              precision    recall  f1-score   support

           0     0.8298    0.9710    0.8948      5000
           1     0.9695    0.8712    0.9177      5000
           2     0.9712    0.9044    0.9366      5000

    accuracy                         0.9155     15000
   macro avg     0.9235    0.9155    0.9164     15000
weighted avg     0.9235    0.9155    0.9164     15000

matrix: 
 [[4855   84   61]
 [ 571 4356   73]
 [ 425   53 4522]]
matrix1: 
 [[97.1   1.68  1.22]
 [11.42 87.12  1.46]
 [ 8.5   1.06 90.44]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.560693411231041   
                Loss on test data: 0.5654972105026245    
                                         
                0.87
              precision    recall  f1-score   support

           0     0.8175    0.9780    0.8905      5000
           1     0.9784    0.8622    0.9166      5000
           2     0.9759    0.9002    0.9365      5000

    accuracy                         0.9135     15000
   macro avg     0.9239    0.9135    0.9146     15000
weighted avg     0.9239    0.9135    0.9146     15000

matrix: 
 [[4890   39   71]
 [ 649 4311   40]
 [ 443   56 4501]]
matrix1: 
 [[97.8   0.78  1.42]
 [12.98 86.22  0.8 ]
 [ 8.86  1.12 90.02]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5664566589593887   
                Loss on test data: 0.5701616146564483    
                                         
                0.87
              precision    recall  f1-score   support

           0     0.8097    0.9542    0.8761      5000
           1     0.9363    0.9002    0.9179      5000
           2     0.9835    0.8460    0.9096      5000

    accuracy                         0.9001     15000
   macro avg     0.9099    0.9001    0.9012     15000
weighted avg     0.9099    0.9001    0.9012     15000

matrix: 
 [[4771  181   48]
 [ 476 4501   23]
 [ 645  125 4230]]
matrix1: 
 [[95.42  3.62  0.96]
 [ 9.52 90.02  0.46]
 [12.9   2.5  84.6 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5607452121973038   
                Loss on test data: 0.5658856066465378    
                                         
                0.87
              precision    recall  f1-score   support

           0     0.8348    0.9572    0.8918      5000
           1     0.9490    0.8900    0.9186      5000
           2     0.9760    0.8936    0.9330      5000

    accuracy                         0.9136     15000
   macro avg     0.9199    0.9136    0.9145     15000
weighted avg     0.9199    0.9136    0.9145     15000

matrix: 
 [[4786  140   74]
 [ 514 4450   36]
 [ 433   99 4468]]
matrix1: 
 [[95.72  2.8   1.48]
 [10.28 89.    0.72]
 [ 8.66  1.98 89.36]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.566297763466835   
                Loss on test data: 0.5705962377786636    
                                         
                0.88
              precision    recall  f1-score   support

           0     0.8096    0.9720    0.8834      5000
           1     0.9560    0.8820    0.9175      5000
           2     0.9797    0.8590    0.9154      5000

    accuracy                         0.9043     15000
   macro avg     0.9151    0.9043    0.9054     15000
weighted avg     0.9151    0.9043    0.9054     15000

matrix: 
 [[4860   81   59]
 [ 560 4410   30]
 [ 583  122 4295]]
matrix1: 
 [[97.2   1.62  1.18]
 [11.2  88.2   0.6 ]
 [11.66  2.44 85.9 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5647670912742615   
                Loss on test data: 0.5696662474870682    
                                         
                0.88
              precision    recall  f1-score   support

           0     0.8036    0.9790    0.8827      5000
           1     0.9722    0.8734    0.9201      5000
           2     0.9828    0.8682    0.9219      5000

    accuracy                         0.9069     15000
   macro avg     0.9195    0.9069    0.9083     15000
weighted avg     0.9195    0.9069    0.9083     15000

matrix: 
 [[4895   55   50]
 [ 607 4367   26]
 [ 589   70 4341]]
matrix1: 
 [[97.9   1.1   1.  ]
 [12.14 87.34  0.52]
 [11.78  1.4  86.82]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5696290541887283   
                Loss on test data: 0.572813153386116    
                                         
                0.88
              precision    recall  f1-score   support

           0     0.8006    0.9626    0.8741      5000
           1     0.9527    0.9032    0.9273      5000
           2     0.9833    0.8354    0.9033      5000

    accuracy                         0.9004     15000
   macro avg     0.9122    0.9004    0.9016     15000
weighted avg     0.9122    0.9004    0.9016     15000

matrix: 
 [[4813  147   40]
 [ 453 4516   31]
 [ 746   77 4177]]
matrix1: 
 [[96.26  2.94  0.8 ]
 [ 9.06 90.32  0.62]
 [14.92  1.54 83.54]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5649212094545364   
                Loss on test data: 0.5697755295038224    
                                         
                0.89
              precision    recall  f1-score   support

           0     0.8160    0.9790    0.8901      5000
           1     0.9788    0.8690    0.9206      5000
           2     0.9794    0.8936    0.9345      5000

    accuracy                         0.9139     15000
   macro avg     0.9247    0.9139    0.9151     15000
weighted avg     0.9247    0.9139    0.9151     15000

matrix: 
 [[4895   44   61]
 [ 622 4345   33]
 [ 482   50 4468]]
matrix1: 
 [[97.9   0.88  1.22]
 [12.44 86.9   0.66]
 [ 9.64  1.   89.36]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5732890267372132   
                Loss on test data: 0.5770879113674163    
                                         
                0.89
              precision    recall  f1-score   support

           0     0.7737    0.9778    0.8639      5000
           1     0.9677    0.8750    0.9190      5000
           2     0.9853    0.8198    0.8950      5000

    accuracy                         0.8909     15000
   macro avg     0.9089    0.8909    0.8926     15000
weighted avg     0.9089    0.8909    0.8926     15000

matrix: 
 [[4889   76   35]
 [ 599 4375   26]
 [ 831   70 4099]]
matrix1: 
 [[97.78  1.52  0.7 ]
 [11.98 87.5   0.52]
 [16.62  1.4  81.98]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5642373459339142   
                Loss on test data: 0.5696763854026794    
                                         
                0.89
              precision    recall  f1-score   support

           0     0.8257    0.9740    0.8937      5000
           1     0.9765    0.8548    0.9116      5000
           2     0.9710    0.9176    0.9435      5000

    accuracy                         0.9155     15000
   macro avg     0.9244    0.9155    0.9163     15000
weighted avg     0.9244    0.9155    0.9163     15000

matrix: 
 [[4870   47   83]
 [ 672 4274   54]
 [ 356   56 4588]]
matrix1: 
 [[97.4   0.94  1.66]
 [13.44 85.48  1.08]
 [ 7.12  1.12 91.76]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5675750476121902   
                Loss on test data: 0.5721739892959594    
                                         
                0.9
              precision    recall  f1-score   support

           0     0.8234    0.9696    0.8905      5000
           1     0.9668    0.8900    0.9268      5000
           2     0.9809    0.8846    0.9303      5000

    accuracy                         0.9147     15000
   macro avg     0.9237    0.9147    0.9159     15000
weighted avg     0.9237    0.9147    0.9159     15000

matrix: 
 [[4848   96   56]
 [ 520 4450   30]
 [ 520   57 4423]]
matrix1: 
 [[96.96  1.92  1.12]
 [10.4  89.    0.6 ]
 [10.4   1.14 88.46]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5703307819366455   
                Loss on test data: 0.57474558198452    
                                         
                0.9
              precision    recall  f1-score   support

           0     0.8324    0.9408    0.8833      5000
           1     0.9225    0.9240    0.9233      5000
           2     0.9855    0.8556    0.9160      5000

    accuracy                         0.9068     15000
   macro avg     0.9135    0.9068    0.9075     15000
weighted avg     0.9135    0.9068    0.9075     15000

matrix: 
 [[4704  255   41]
 [ 358 4620   22]
 [ 589  133 4278]]
matrix1: 
 [[94.08  5.1   0.82]
 [ 7.16 92.4   0.44]
 [11.78  2.66 85.56]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5688240694999694   
                Loss on test data: 0.5738071792125702    
                                         
                0.9
              precision    recall  f1-score   support

           0     0.8076    0.9788    0.8850      5000
           1     0.9749    0.8636    0.9159      5000
           2     0.9818    0.8858    0.9313      5000

    accuracy                         0.9094     15000
   macro avg     0.9215    0.9094    0.9107     15000
weighted avg     0.9215    0.9094    0.9107     15000

matrix: 
 [[4894   57   49]
 [ 649 4318   33]
 [ 517   54 4429]]
matrix1: 
 [[97.88  1.14  0.98]
 [12.98 86.36  0.66]
 [10.34  1.08 88.58]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5702499790191651   
                Loss on test data: 0.5754686601161957    
                                         
                0.91
              precision    recall  f1-score   support

           0     0.8130    0.9750    0.8867      5000
           1     0.9739    0.8658    0.9167      5000
           2     0.9778    0.8916    0.9327      5000

    accuracy                         0.9108     15000
   macro avg     0.9216    0.9108    0.9120     15000
weighted avg     0.9216    0.9108    0.9120     15000

matrix: 
 [[4875   60   65]
 [ 635 4329   36]
 [ 486   56 4458]]
matrix1: 
 [[97.5   1.2   1.3 ]
 [12.7  86.58  0.72]
 [ 9.72  1.12 89.16]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5788974251747131   
                Loss on test data: 0.5821325222253799    
                                         
                0.91
              precision    recall  f1-score   support

           0     0.7701    0.9844    0.8642      5000
           1     0.9735    0.8732    0.9206      5000
           2     0.9864    0.8136    0.8917      5000

    accuracy                         0.8904     15000
   macro avg     0.9100    0.8904    0.8922     15000
weighted avg     0.9100    0.8904    0.8922     15000

matrix: 
 [[4922   42   36]
 [ 614 4366   20]
 [ 855   77 4068]]
matrix1: 
 [[98.44  0.84  0.72]
 [12.28 87.32  0.4 ]
 [17.1   1.54 81.36]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5728823778629303   
                Loss on test data: 0.5776645839214325    
                                         
                0.91
              precision    recall  f1-score   support

           0     0.8061    0.9744    0.8823      5000
           1     0.9693    0.8512    0.9064      5000
           2     0.9768    0.8918    0.9324      5000

    accuracy                         0.9058     15000
   macro avg     0.9174    0.9058    0.9070     15000
weighted avg     0.9174    0.9058    0.9070     15000

matrix: 
 [[4872   65   63]
 [ 701 4256   43]
 [ 471   70 4459]]
matrix1: 
 [[97.44  1.3   1.26]
 [14.02 85.12  0.86]
 [ 9.42  1.4  89.18]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5767152043581009   
                Loss on test data: 0.5806654011011123    
                                         
                0.92
              precision    recall  f1-score   support

           0     0.8069    0.9634    0.8782      5000
           1     0.9528    0.8972    0.9242      5000
           2     0.9826    0.8494    0.9112      5000

    accuracy                         0.9033     15000
   macro avg     0.9141    0.9033    0.9045     15000
weighted avg     0.9141    0.9033    0.9045     15000

matrix: 
 [[4817  138   45]
 [ 484 4486   30]
 [ 669   84 4247]]
matrix1: 
 [[96.34  2.76  0.9 ]
 [ 9.68 89.72  0.6 ]
 [13.38  1.68 84.94]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5731440916061401   
                Loss on test data: 0.5766105804443359    
                                         
                0.92
              precision    recall  f1-score   support

           0     0.8540    0.9344    0.8924      5000
           1     0.9215    0.9254    0.9235      5000
           2     0.9809    0.8844    0.9302      5000

    accuracy                         0.9147     15000
   macro avg     0.9188    0.9147    0.9153     15000
weighted avg     0.9188    0.9147    0.9153     15000

matrix: 
 [[4672  269   59]
 [ 346 4627   27]
 [ 453  125 4422]]
matrix1: 
 [[93.44  5.38  1.18]
 [ 6.92 92.54  0.54]
 [ 9.06  2.5  88.44]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5774557783603668   
                Loss on test data: 0.581961445093155    
                                         
                0.92
              precision    recall  f1-score   support

           0     0.7806    0.9856    0.8712      5000
           1     0.9879    0.8324    0.9035      5000
           2     0.9783    0.8754    0.9240      5000

    accuracy                         0.8978     15000
   macro avg     0.9156    0.8978    0.8996     15000
weighted avg     0.9156    0.8978    0.8996     15000

matrix: 
 [[4928   22   50]
 [ 791 4162   47]
 [ 594   29 4377]]
matrix1: 
 [[98.56  0.44  1.  ]
 [15.82 83.24  0.94]
 [11.88  0.58 87.54]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5755243180990219   
                Loss on test data: 0.5809491360187531    
                                         
                0.93
              precision    recall  f1-score   support

           0     0.8076    0.9748    0.8834      5000
           1     0.9686    0.8870    0.9260      5000
           2     0.9834    0.8626    0.9190      5000

    accuracy                         0.9081     15000
   macro avg     0.9198    0.9081    0.9095     15000
weighted avg     0.9198    0.9081    0.9095     15000

matrix: 
 [[4874   79   47]
 [ 539 4435   26]
 [ 622   65 4313]]
matrix1: 
 [[97.48  1.58  0.94]
 [10.78 88.7   0.52]
 [12.44  1.3  86.26]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5840106021165847   
                Loss on test data: 0.587599935412407    
                                         
                0.93
              precision    recall  f1-score   support

           0     0.7765    0.9756    0.8647      5000
           1     0.9535    0.8858    0.9184      5000
           2     0.9865    0.8036    0.8857      5000

    accuracy                         0.8883     15000
   macro avg     0.9055    0.8883    0.8896     15000
weighted avg     0.9055    0.8883    0.8896     15000

matrix: 
 [[4878   89   33]
 [ 549 4429   22]
 [ 855  127 4018]]
matrix1: 
 [[97.56  1.78  0.66]
 [10.98 88.58  0.44]
 [17.1   2.54 80.36]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.586650397658348   
                Loss on test data: 0.5909795767068863    
                                         
                0.93
              precision    recall  f1-score   support

           0     0.8480    0.9172    0.8812      5000
           1     0.9015    0.8892    0.8953      5000
           2     0.9775    0.9110    0.9431      5000

    accuracy                         0.9058     15000
   macro avg     0.9090    0.9058    0.9065     15000
weighted avg     0.9090    0.9058    0.9065     15000

matrix: 
 [[4586  346   68]
 [ 517 4446   37]
 [ 305  140 4555]]
matrix1: 
 [[91.72  6.92  1.36]
 [10.34 88.92  0.74]
 [ 6.1   2.8  91.1 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5785982924699783   
                Loss on test data: 0.5833930555582046    
                                         
                0.94
              precision    recall  f1-score   support

           0     0.8147    0.9630    0.8827      5000
           1     0.9551    0.8930    0.9230      5000
           2     0.9823    0.8674    0.9213      5000

    accuracy                         0.9078     15000
   macro avg     0.9174    0.9078    0.9090     15000
weighted avg     0.9174    0.9078    0.9090     15000

matrix: 
 [[4815  137   48]
 [ 505 4465   30]
 [ 590   73 4337]]
matrix1: 
 [[96.3   2.74  0.96]
 [10.1  89.3   0.6 ]
 [11.8   1.46 86.74]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5811875771284103   
                Loss on test data: 0.5850581240653991    
                                         
                0.94
              precision    recall  f1-score   support

           0     0.8022    0.9752    0.8803      5000
           1     0.9615    0.8896    0.9242      5000
           2     0.9856    0.8468    0.9109      5000

    accuracy                         0.9039     15000
   macro avg     0.9164    0.9039    0.9051     15000
weighted avg     0.9164    0.9039    0.9051     15000

matrix: 
 [[4876   86   38]
 [ 528 4448   24]
 [ 674   92 4234]]
matrix1: 
 [[97.52  1.72  0.76]
 [10.56 88.96  0.48]
 [13.48  1.84 84.68]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5752191712856293   
                Loss on test data: 0.5806040104627609    
                                         
                0.94
              precision    recall  f1-score   support

           0     0.8477    0.9454    0.8939      5000
           1     0.9363    0.9058    0.9208      5000
           2     0.9780    0.8972    0.9359      5000

    accuracy                         0.9161     15000
   macro avg     0.9207    0.9161    0.9169     15000
weighted avg     0.9207    0.9161    0.9169     15000

matrix: 
 [[4727  206   67]
 [ 437 4529   34]
 [ 412  102 4486]]
matrix1: 
 [[94.54  4.12  1.34]
 [ 8.74 90.58  0.68]
 [ 8.24  2.04 89.72]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5835132476091385   
                Loss on test data: 0.5885248303413391    
                                         
                0.95
              precision    recall  f1-score   support

           0     0.8119    0.9468    0.8742      5000
           1     0.9309    0.8894    0.9097      5000
           2     0.9847    0.8650    0.9210      5000

    accuracy                         0.9004     15000
   macro avg     0.9092    0.9004    0.9016     15000
weighted avg     0.9092    0.9004    0.9016     15000

matrix: 
 [[4734  225   41]
 [ 527 4447   26]
 [ 570  105 4325]]
matrix1: 
 [[94.68  4.5   0.82]
 [10.54 88.94  0.52]
 [11.4   2.1  86.5 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.580656258225441   
                Loss on test data: 0.5859964997768402    
                                         
                0.95
              precision    recall  f1-score   support

           0     0.8017    0.9794    0.8817      5000
           1     0.9832    0.8450    0.9089      5000
           2     0.9752    0.8962    0.9340      5000

    accuracy                         0.9069     15000
   macro avg     0.9201    0.9069    0.9082     15000
weighted avg     0.9201    0.9069    0.9082     15000

matrix: 
 [[4897   34   69]
 [ 730 4225   45]
 [ 481   38 4481]]
matrix1: 
 [[97.94  0.68  1.38]
 [14.6  84.5   0.9 ]
 [ 9.62  0.76 89.62]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5796637332439423   
                Loss on test data: 0.5838281517028808    
                                         
                0.95
              precision    recall  f1-score   support

           0     0.8373    0.9544    0.8920      5000
           1     0.9411    0.9114    0.9260      5000
           2     0.9823    0.8760    0.9261      5000

    accuracy                         0.9139     15000
   macro avg     0.9203    0.9139    0.9147     15000
weighted avg     0.9203    0.9139    0.9147     15000

matrix: 
 [[4772  174   54]
 [ 418 4557   25]
 [ 509  111 4380]]
matrix1: 
 [[95.44  3.48  1.08]
 [ 8.36 91.14  0.5 ]
 [10.18  2.22 87.6 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.583658639550209   
                Loss on test data: 0.5890735331773758    
                                         
                0.96
              precision    recall  f1-score   support

           0     0.8438    0.9184    0.8795      5000
           1     0.9045    0.9188    0.9116      5000
           2     0.9801    0.8780    0.9263      5000

    accuracy                         0.9051     15000
   macro avg     0.9095    0.9051    0.9058     15000
weighted avg     0.9095    0.9051    0.9058     15000

matrix: 
 [[4592  352   56]
 [ 373 4594   33]
 [ 477  133 4390]]
matrix1: 
 [[91.84  7.04  1.12]
 [ 7.46 91.88  0.66]
 [ 9.54  2.66 87.8 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5799378514289856   
                Loss on test data: 0.5850374221801757    
                                         
                0.96
              precision    recall  f1-score   support

           0     0.8322    0.9672    0.8946      5000
           1     0.9627    0.8920    0.9260      5000
           2     0.9796    0.8926    0.9341      5000

    accuracy                         0.9173     15000
   macro avg     0.9248    0.9173    0.9182     15000
weighted avg     0.9248    0.9173    0.9182     15000

matrix: 
 [[4836  102   62]
 [ 509 4460   31]
 [ 466   71 4463]]
matrix1: 
 [[96.72  2.04  1.24]
 [10.18 89.2   0.62]
 [ 9.32  1.42 89.26]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.58491827917099   
                Loss on test data: 0.5891397030353546    
                                         
                0.96
              precision    recall  f1-score   support

           0     0.7933    0.9830    0.8780      5000
           1     0.9822    0.8502    0.9114      5000
           2     0.9799    0.8772    0.9257      5000

    accuracy                         0.9035     15000
   macro avg     0.9185    0.9035    0.9050     15000
weighted avg     0.9185    0.9035    0.9050     15000

matrix: 
 [[4915   34   51]
 [ 710 4251   39]
 [ 571   43 4386]]
matrix1: 
 [[98.3   0.68  1.02]
 [14.2  85.02  0.78]
 [11.42  0.86 87.72]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.582450059890747   
                Loss on test data: 0.5871180099248886    
                                         
                0.97
              precision    recall  f1-score   support

           0     0.8294    0.9750    0.8963      5000
           1     0.9726    0.8744    0.9209      5000
           2     0.9786    0.9056    0.9407      5000

    accuracy                         0.9183     15000
   macro avg     0.9269    0.9183    0.9193     15000
weighted avg     0.9269    0.9183    0.9193     15000

matrix: 
 [[4875   58   67]
 [ 596 4372   32]
 [ 407   65 4528]]
matrix1: 
 [[97.5   1.16  1.34]
 [11.92 87.44  0.64]
 [ 8.14  1.3  90.56]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.583137533903122   
                Loss on test data: 0.5881722532510757    
                                         
                0.97
              precision    recall  f1-score   support

           0     0.8224    0.9760    0.8926      5000
           1     0.9735    0.8760    0.9222      5000
           2     0.9794    0.8946    0.9351      5000

    accuracy                         0.9155     15000
   macro avg     0.9251    0.9155    0.9166     15000
weighted avg     0.9251    0.9155    0.9166     15000

matrix: 
 [[4880   64   56]
 [ 582 4380   38]
 [ 472   55 4473]]
matrix1: 
 [[97.6   1.28  1.12]
 [11.64 87.6   0.76]
 [ 9.44  1.1  89.46]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5856420032978058   
                Loss on test data: 0.5906445434093476    
                                         
                0.97
              precision    recall  f1-score   support

           0     0.8240    0.9524    0.8836      5000
           1     0.9394    0.8954    0.9169      5000
           2     0.9811    0.8742    0.9246      5000

    accuracy                         0.9073     15000
   macro avg     0.9148    0.9073    0.9083     15000
weighted avg     0.9148    0.9073    0.9083     15000

matrix: 
 [[4762  185   53]
 [ 492 4477   31]
 [ 525  104 4371]]
matrix1: 
 [[95.24  3.7   1.06]
 [ 9.84 89.54  0.62]
 [10.5   2.08 87.42]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5846342451572418   
                Loss on test data: 0.5904335594177246    
                                         
                0.98
              precision    recall  f1-score   support

           0     0.8543    0.9314    0.8912      5000
           1     0.9240    0.8972    0.9104      5000
           2     0.9734    0.9138    0.9426      5000

    accuracy                         0.9141     15000
   macro avg     0.9172    0.9141    0.9148     15000
weighted avg     0.9172    0.9141    0.9148     15000

matrix: 
 [[4657  256   87]
 [ 476 4486   38]
 [ 318  113 4569]]
matrix1: 
 [[93.14  5.12  1.74]
 [ 9.52 89.72  0.76]
 [ 6.36  2.26 91.38]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5809169074296952   
                Loss on test data: 0.586426206946373    
                                         
                0.98
              precision    recall  f1-score   support

           0     0.8684    0.9546    0.9095      5000
           1     0.9686    0.8940    0.9298      5000
           2     0.9601    0.9388    0.9493      5000

    accuracy                         0.9291     15000
   macro avg     0.9324    0.9291    0.9295     15000
weighted avg     0.9324    0.9291    0.9295     15000

matrix: 
 [[4773   78  149]
 [ 484 4470   46]
 [ 239   67 4694]]
matrix1: 
 [[95.46  1.56  2.98]
 [ 9.68 89.4   0.92]
 [ 4.78  1.34 93.88]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5870385247468949   
                Loss on test data: 0.5914752267599106    
                                         
                0.98
              precision    recall  f1-score   support

           0     0.8129    0.9776    0.8877      5000
           1     0.9702    0.8800    0.9229      5000
           2     0.9841    0.8762    0.9270      5000

    accuracy                         0.9113     15000
   macro avg     0.9224    0.9113    0.9125     15000
weighted avg     0.9224    0.9113    0.9125     15000

matrix: 
 [[4888   65   47]
 [ 576 4400   24]
 [ 549   70 4381]]
matrix1: 
 [[97.76  1.3   0.94]
 [11.52 88.    0.48]
 [10.98  1.4  87.62]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5909787917137146   
                Loss on test data: 0.595666265964508    
                                         
                0.99
              precision    recall  f1-score   support

           0     0.8123    0.9728    0.8853      5000
           1     0.9661    0.8554    0.9074      5000
           2     0.9782    0.8970    0.9358      5000

    accuracy                         0.9084     15000
   macro avg     0.9189    0.9084    0.9095     15000
weighted avg     0.9189    0.9084    0.9095     15000

matrix: 
 [[4864   74   62]
 [ 685 4277   38]
 [ 439   76 4485]]
matrix1: 
 [[97.28  1.48  1.24]
 [13.7  85.54  0.76]
 [ 8.78  1.52 89.7 ]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5982154746055603   
                Loss on test data: 0.6016886183023453    
                                         
                0.99
              precision    recall  f1-score   support

           0     0.7670    0.9784    0.8599      5000
           1     0.9625    0.8736    0.9159      5000
           2     0.9878    0.8068    0.8882      5000

    accuracy                         0.8863     15000
   macro avg     0.9058    0.8863    0.8880     15000
weighted avg     0.9058    0.8863    0.8880     15000

matrix: 
 [[4892   76   32]
 [ 614 4368   18]
 [ 872   94 4034]]
matrix1: 
 [[97.84  1.52  0.64]
 [12.28 87.36  0.36]
 [17.44  1.88 80.68]]
                         

                			 Loss : FPTP         
                Parameter: 0.0002            
                Loss on train data: 0.5844095383882523   
                Loss on test data: 0.5895599300861358    
                                         
                0.99
              precision    recall  f1-score   support

           0     0.8643    0.9476    0.9040      5000
           1     0.9431    0.9120    0.9273      5000
           2     0.9765    0.9146    0.9445      5000

    accuracy                         0.9247     15000
   macro avg     0.9280    0.9247    0.9253     15000
weighted avg     0.9280    0.9247    0.9253     15000

matrix: 
 [[4738  186   76]
 [ 406 4560   34]
 [ 338   89 4573]]
matrix1: 
 [[94.76  3.72  1.52]
 [ 8.12 91.2   0.68]
 [ 6.76  1.78 91.46]]
